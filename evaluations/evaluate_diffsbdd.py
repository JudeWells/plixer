import os
import glob
import pandas as pd
import sys
import subprocess
import argparse
import json
from rdkit import Chem, DataStructs
from rdkit.Chem import AllChem
from rdkit.Chem import rdFMCS

from evaluations.evaluate_combined_vox2smiles import (
    get_tanimoto_similarity_from_smiles,
    get_max_common_substructure_num_atoms,
    calculate_enrichment_factor,
)

"""
For each test-set system 
Example command to run DiffSBDD:
conda activate sbdd-env && python generate_ligands.py checkpoints/crossdocked_fullatom_cond.ckpt --pdbfile {pdb_path} --outfile {outdir}/{system_id}.sdf --ref_ligand {ligand_path} --n_samples 1

JW NOTE: this script was copied and executed from the diffSBDD directory
"""


def summarize_results(results_df, output_dir=None):
    """Summarize the evaluation results."""
    if len(results_df) == 0:
        print("No valid results to summarize")
        return {}
    
    # Calculate metrics
    results = dict(
        validity = results_df.valid_smiles.mean(),
        tanimoto_similarity = results_df.tanimoto_similarity.mean(),
        decoy_tanimoto_similarity = results_df.decoy_tanimoto_similarity.mean() if 'decoy_tanimoto_similarity' in results_df.columns else None,
        n_mols_gt_0p3_tanimoto = len(results_df[results_df.tanimoto_similarity >= 0.3]),
        tanimoto_enrichment = calculate_enrichment_factor(results_df, target_col="tanimoto_similarity", target_threshold=0.3) 
                              if 'decoy_tanimoto_similarity' in results_df.columns else None,
        proportion_mols_are_unique = len(results_df.sampled_smiles.unique()) / len(results_df)
    )
    
    # Round and clean up results
    results = {k: round(v, 4) if isinstance(v, (int, float)) and not pd.isna(v) else v 
               for k, v in results.items() if v is not None}
    
    # Print results
    for k, v in results.items():
        print(f"{k}: {v}")
        
    # Save results if output directory is provided
    if output_dir is not None:
        result_savepath = f"{output_dir}/results_summary.json"
        with open(result_savepath, 'w') as filehandle:
            json.dump(results, filehandle, indent=2)
            
    return results

def analyse_diffsbdd_prediction(sdf_filepath):
    """Analyze the SDF file generated by DiffSBDD and return metrics."""
    metrics = {}
    
    # Check if file exists
    if not os.path.exists(sdf_filepath):
        print(f"Warning: SDF file does not exist: {sdf_filepath}")
        metrics = {
            "valid_smiles": 0,
            "tanimoto_similarity": None,
            "decoy_tanimoto_similarity": None,
            "true_vs_decoy_tanimoto_similarity": None,
            "mcs_num_atoms": None,
            "decoy_mcs_num_atoms": None,
            "true_vs_decoy_mcs_num_atoms": None,
            "prop_common_structure": None,
            "decoy_prop_common_structure": None,
            "sampled_smiles": None,
            "generated_molecule_valid": False
        }
        return metrics
    
    try:
        # Read the SDF file with RDKit
        suppl = Chem.SDMolSupplier(sdf_filepath)
        
        # Get the true ligand SMILES
        system_id = os.path.basename(sdf_filepath).split(".sdf")[0]
        ligand_path = f"/mnt/disk2/VoxelDiffOuter/hiqbind/test_pdbs/{system_id}/{system_id}_ligand_refined.sdf"
        
        # Check if the ref ligand file exists
        if not os.path.exists(ligand_path):
            print(f"Warning: Reference ligand file does not exist: {ligand_path}")
            metrics = {
                "valid_smiles": 0,
                "sampled_smiles": None,
                "generated_molecule_valid": False
            }
            return metrics
            
        # Read the true ligand
        true_suppl = Chem.SDMolSupplier(ligand_path)
        true_mol = next(mol for mol in true_suppl if mol is not None)
        true_smiles = Chem.MolToSmiles(true_mol)
        
        # Get generated molecule (take the first valid molecule from the SDF)
        generated_mol = None
        for mol in suppl:
            if mol is not None:
                generated_mol = mol
                break
        
        # Process the molecule
        if generated_mol is not None:
            # Convert to SMILES
            sampled_smiles = Chem.MolToSmiles(generated_mol)
            valid_smiles = 1
            
            # Get true molecule details
            true_num_heavy_atoms = true_mol.GetNumHeavyAtoms()
            
            # Calculate Tanimoto similarity
            tanimoto_similarity = get_tanimoto_similarity_from_smiles(true_smiles, sampled_smiles)
            
            # Calculate MCS
            mcs_num_atoms = get_max_common_substructure_num_atoms(true_smiles, sampled_smiles)
            
            # Calculate proportion of common structure
            if mcs_num_atoms is not None and true_num_heavy_atoms > 0:
                prop_common_structure = mcs_num_atoms / true_num_heavy_atoms
            else:
                prop_common_structure = None
                
            # Return metrics
            metrics = {
                "valid_smiles": valid_smiles,
                "sampled_smiles": sampled_smiles,
                "tanimoto_similarity": tanimoto_similarity,
                "mcs_num_atoms": mcs_num_atoms,
                "true_num_heavy_atoms": true_num_heavy_atoms,
                "prop_common_structure": prop_common_structure,
                "generated_molecule_valid": True,
                "true_smiles": true_smiles
            }
        else:
            # Invalid molecule
            metrics = {
                "valid_smiles": 0,
                "sampled_smiles": None,
                "generated_molecule_valid": False,
                "true_smiles": true_smiles if 'true_mol' in locals() else None
            }
    except Exception as e:
        print(f"Error analyzing SDF file {sdf_filepath}: {e}")
        metrics = {
            "valid_smiles": 0,
            "sampled_smiles": None,
            "generated_molecule_valid": False,
            "error": str(e)
        }
    
    return metrics

if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Run DiffSBDD on test set proteins')
    parser.add_argument('--diffsbdd_dir', type=str, default='/mnt/disk2/DiffSBDD',
                        help='Path to DiffSBDD directory containing generate_ligands.py')
    parser.add_argument('--checkpoint', type=str, default='checkpoints/crossdocked_fullatom_cond.ckpt',
                        help='Path to DiffSBDD checkpoint file')
    parser.add_argument('--outdir', type=str, default='/mnt/disk2/VoxelDiffOuter/VoxelDiff2/evaluation_results/diffsbdd_results',
                        help='Directory to save generated ligands')
    parser.add_argument('--n_samples', type=int, default=1,
                        help='Number of samples to generate per protein')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from previous run if results file exists')
    parser.add_argument('--analyse_only', action='store_true',
                        help='Only analyze existing results without running DiffSBDD')
    parser.add_argument('--plixer_csv', type=str,
                        default="evaluation_results/bubba_zjhnye4j_2025-05-11_highPropPoc2Mol/combined_model_results.csv",
                        help='Path to Plixer CSV file with reference molecule ordering')
    args = parser.parse_args()
    
    # Define output directory
    outdir = args.outdir
    os.makedirs(outdir, exist_ok=True)
    
    # Get all protein PDB files in test set
    pdb_paths = glob.glob("/mnt/disk2/VoxelDiffOuter/hiqbind/test_pdbs/*/*protein_refined.pdb")
    
    # Track success/failure of each run
    results = []
    completed_systems = set()
    
    # Check for existing results if resuming
    results_file = f"{outdir}/diffsbdd_run_results.csv"
    if (args.resume or args.analyse_only) and os.path.exists(results_file):
        print(f"Loading existing results from {results_file}")
        existing_results = pd.read_csv(results_file)
        results = existing_results.to_dict('records')
        
        # Get list of completed systems
        for result in results:
            completed_systems.add(result['system_id'])
        
        print(f"Found {len(completed_systems)} already processed systems")
    
    # If analyse_only, just process the existing SDF files
    if args.analyse_only:
        sdf_files = glob.glob(f"{outdir}/*.sdf")
        
        for sdf_file in sdf_files:
            system_id = os.path.basename(sdf_file).split(".sdf")[0]
            
            # Skip if already analyzed
            if system_id in completed_systems:
                print(f"Skipping already analyzed system: {system_id}")
                continue
                
            print(f"Analyzing: {system_id}")
            
            # Get corresponding paths
            pdb_path = next((p for p in pdb_paths if system_id in p), None)
            if pdb_path is None:
                print(f"Could not find PDB path for system: {system_id}")
                continue
                
            ligand_path = pdb_path.replace("protein_refined.pdb", "ligand_refined.sdf")
            
            # Analyze the SDF file
            metrics = analyse_diffsbdd_prediction(sdf_file)
            metrics.update({
                "system_id": system_id,
                "pdb_path": pdb_path,
                "ligand_path": ligand_path,
                "status": "success" if metrics.get("valid_smiles", 0) == 1 else "failed"
            })
            
            results.append(metrics)
            
            # Save results after each analysis
            results_df = pd.DataFrame(results)
            results_df.to_csv(results_file, index=False)
    else:
        # Process each protein
        for pdb_path in pdb_paths:
            # Extract system ID from filename
            system_id = os.path.basename(pdb_path).split("_protein_refined.pdb")[0]
            
            # Skip if already processed and resuming
            if system_id in completed_systems:
                print(f"Skipping already processed system: {system_id}")
                continue
            
            # Get corresponding ligand path
            ligand_path = pdb_path.replace("protein_refined.pdb", "ligand_refined.sdf")
            assert os.path.exists(ligand_path)
            print(f"Processing system: {system_id}")
            
            # Get checkpoint path relative to DiffSBDD directory
            checkpoint_rel_path = args.checkpoint
            
            # Construct command to run DiffSBDD
            # Change directory to DiffSBDD directory first, then run the command
            outfile_path = f"{outdir}/{system_id}.sdf"
            status = "failed"
            
            if not os.path.exists(outfile_path):
                try:
                    # Execute the command
                    cmd = f"bash -l -c 'cd {args.diffsbdd_dir} && conda activate sbdd-env && python generate_ligands.py {checkpoint_rel_path} --pdbfile {pdb_path} --outfile {outfile_path} --ref_ligand {ligand_path} --n_samples {args.n_samples}'"
                    result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                    status = "success"
                    print(f"Successfully generated ligands for {system_id}")
                except subprocess.CalledProcessError as e:
                    status = "failed"
                    print(f"Error generating ligands for {system_id}: {str(e)}")
                    print(f"STDOUT: {e.stdout.decode('utf-8')}")
                    print(f"STDERR: {e.stderr.decode('utf-8')}")
            
            # Analyze the generated SDF file
            metrics = analyse_diffsbdd_prediction(outfile_path)
            metrics.update({
                "system_id": system_id,
                "pdb_path": pdb_path,
                "ligand_path": ligand_path,
                "status": status if not os.path.exists(outfile_path) else ("success" if metrics.get("valid_smiles", 0) == 1 else "failed")
            })
            
            results.append(metrics)
            
            # Save results after each run in case of interruption
            results_df = pd.DataFrame(results)
            results_df.to_csv(results_file, index=False)
    
    # Final results analysis
    results_df = pd.DataFrame(results)
    success_count = results_df[results_df['status'] == 'success'].shape[0]
    valid_count = results_df[results_df['valid_smiles'] == 1].shape[0]
    failed_count = results_df[results_df['status'] == 'failed'].shape[0]
    
    print(f"\nCompleted processing {len(results)} systems.")
    print(f"Success: {success_count}")
    print(f"Valid molecules: {valid_count}")
    print(f"Failed: {failed_count}")
    print(f"Results saved to {results_file}")
    
    # Load Plixer CSV to get the consistent ordering of decoy-active pairs
    try:
        plixer_df = pd.read_csv(args.plixer_csv)
        print(f"Loaded Plixer reference file with {len(plixer_df)} entries")
        
        # Make sure we have the same systems in both dataframes
        system_order = plixer_df['name'].tolist()
        
        # Create a mapping of system IDs to their indices in the system_order list
        system_order_map = {system_id: idx for idx, system_id in enumerate(system_order)}
        
        # Add order column to results_df
        results_df['order'] = results_df['system_id'].map(lambda x: system_order_map.get(x, float('inf')))
        
        # Sort by the order column
        results_df = results_df.sort_values('order').reset_index(drop=True)
        
        # Drop systems not in the reference list and the order column
        results_df = results_df[results_df['order'] != float('inf')]
        
        print(f"Matched {len(results_df)} systems with Plixer reference")
        
        # Ensure that all systems from plixer_df are in results_df, even if they failed
        missing_systems = set(system_order) - set(results_df['system_id'])
        if missing_systems:
            print(f"Adding {len(missing_systems)} missing systems from reference list")
            missing_rows = []
            
            for system_id in missing_systems:
                # Get reference SMILES for the system
                ref_row = plixer_df[plixer_df['name'] == system_id].iloc[0]
                true_smiles = ref_row['smiles'].replace("[BOS]", "").replace("[EOS]", "")
                
                # Create a placeholder row for the missing system
                missing_rows.append({
                    "system_id": system_id,
                    "status": "failed",
                    "valid_smiles": 0,
                    "sampled_smiles": None,
                    "true_smiles": true_smiles,
                    "order": system_order_map[system_id]
                })
            
            # Add the missing rows to results_df
            missing_df = pd.DataFrame(missing_rows)
            results_df = pd.concat([results_df, missing_df]).sort_values('order').reset_index(drop=True)
        
        # Now create decoy relationships using the order from plixer_df
        results_df['decoy_system_id'] = results_df['system_id'].shift(1)
        results_df['decoy_smiles'] = results_df['true_smiles'].shift(1)
        
        # Calculate decoy metrics for valid molecules
        for idx, row in results_df.iterrows():
            if row['valid_smiles'] == 1 and not pd.isna(row['decoy_smiles']):
                # Calculate decoy Tanimoto similarity
                decoy_tanimoto = get_tanimoto_similarity_from_smiles(row['decoy_smiles'], row['sampled_smiles'])
                results_df.at[idx, 'decoy_tanimoto_similarity'] = decoy_tanimoto
                
                # Calculate true vs decoy Tanimoto similarity
                true_vs_decoy = get_tanimoto_similarity_from_smiles(row['true_smiles'], row['decoy_smiles'])
                results_df.at[idx, 'true_vs_decoy_tanimoto_similarity'] = true_vs_decoy
                
                # Calculate decoy MCS and related metrics
                decoy_mcs = get_max_common_substructure_num_atoms(row['decoy_smiles'], row['sampled_smiles'])
                results_df.at[idx, 'decoy_mcs_num_atoms'] = decoy_mcs
                
                true_vs_decoy_mcs = get_max_common_substructure_num_atoms(row['true_smiles'], row['decoy_smiles'])
                results_df.at[idx, 'true_vs_decoy_mcs_num_atoms'] = true_vs_decoy_mcs
                
                # Calculate decoy proportion of common structure
                if decoy_mcs is not None and not pd.isna(row.get('true_num_heavy_atoms')) and row.get('true_num_heavy_atoms', 0) > 0:
                    results_df.at[idx, 'decoy_prop_common_structure'] = decoy_mcs / row['true_num_heavy_atoms']
        
        # Save results with consistent decoy assignments
        results_df.to_csv(f"{outdir}/diffsbdd_results_with_consistent_decoys.csv", index=False)
        
        # Get statistics for valid molecules
        valid_results = results_df[results_df['valid_smiles'] == 1].copy()
        
        if len(valid_results) > 1:
            print(f"Calculating statistics for {len(valid_results)} valid molecules")
            
            # Calculate summary metrics including enrichment factors
            summary = summarize_results(valid_results, outdir)
        else:
            print("Not enough valid results to calculate statistics")
            
    except Exception as e:
        print(f"Error processing results with Plixer reference: {e}")
        
        # Fall back to original calculation method
        valid_results = results_df[results_df['valid_smiles'] == 1].copy()
        
        if len(valid_results) > 1:
            # Shift the true SMILES to create decoys (each molecule uses previous molecule as decoy)
            valid_results['decoy_smiles'] = valid_results['true_smiles'].shift(1)
            
            # Calculate decoy metrics
            for idx, row in valid_results.iterrows():
                if pd.isna(row['decoy_smiles']):
                    continue
                    
                # Calculate decoy metrics as before
                decoy_tanimoto = get_tanimoto_similarity_from_smiles(row['decoy_smiles'], row['sampled_smiles'])
                valid_results.at[idx, 'decoy_tanimoto_similarity'] = decoy_tanimoto
                
                true_vs_decoy = get_tanimoto_similarity_from_smiles(row['true_smiles'], row['decoy_smiles'])
                valid_results.at[idx, 'true_vs_decoy_tanimoto_similarity'] = true_vs_decoy
                
                decoy_mcs = get_max_common_substructure_num_atoms(row['decoy_smiles'], row['sampled_smiles'])
                valid_results.at[idx, 'decoy_mcs_num_atoms'] = decoy_mcs
                
                true_vs_decoy_mcs = get_max_common_substructure_num_atoms(row['true_smiles'], row['decoy_smiles'])
                valid_results.at[idx, 'true_vs_decoy_mcs_num_atoms'] = true_vs_decoy_mcs
                
                if decoy_mcs is not None and not pd.isna(row['true_num_heavy_atoms']) and row['true_num_heavy_atoms'] > 0:
                    valid_results.at[idx, 'decoy_prop_common_structure'] = decoy_mcs / row['true_num_heavy_atoms']
            
            # Save the fallback results
            valid_results.to_csv(f"{outdir}/diffsbdd_valid_results_with_decoys.csv", index=False)
            
            # Summarize results
            summary = summarize_results(valid_results, outdir)
        else:
            print("Not enough valid results to calculate decoy metrics")
